{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maruanottoni/miniforge3/envs/gce/lib/python3.10/site-packages/torch_geometric/typing.py:47: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: dlopen(/Users/maruanottoni/miniforge3/envs/gce/lib/python3.10/site-packages/libpyg.so, 0x0006): Symbol not found: __ZN2at4_ops11randint_low4callEN3c106SymIntES3_NS2_8ArrayRefIS3_EENS2_8optionalINS2_10ScalarTypeEEENS6_INS2_6LayoutEEENS6_INS2_6DeviceEEENS6_IbEE\n",
      "  Referenced from: <6FC6862C-80A9-3302-B25C-C2758B957C4A> /Users/maruanottoni/miniforge3/envs/gce/lib/python3.10/site-packages/libpyg.so\n",
      "  Expected in:     <78DA4C67-8BC7-3995-8324-D2FD95E57BA4> /Users/maruanottoni/miniforge3/envs/gce/lib/python3.10/site-packages/torch/lib/libtorch_cpu.dylib\n",
      "  warnings.warn(f\"An issue occurred while importing 'pyg-lib'. \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import VGAE, GCNConv\n",
    "from torch_geometric.utils import from_networkx, erdos_renyi_graph\n",
    "import networkx as nx\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mappingproxy({'__module__': 'torch_geometric.nn.models.autoencoder',\n",
       "              '__doc__': 'The Variational Graph Auto-Encoder model from the\\n    `\"Variational Graph Auto-Encoders\" <https://arxiv.org/abs/1611.07308>`_\\n    paper.\\n\\n    Args:\\n        encoder (torch.nn.Module): The encoder module to compute :math:`\\\\mu`\\n            and :math:`\\\\log\\\\sigma^2`.\\n        decoder (torch.nn.Module, optional): The decoder module. If set to\\n            :obj:`None`, will default to the\\n            :class:`torch_geometric.nn.models.InnerProductDecoder`.\\n            (default: :obj:`None`)\\n    ',\n",
       "              '__init__': <function torch_geometric.nn.models.autoencoder.VGAE.__init__(self, encoder: torch.nn.modules.module.Module, decoder: Optional[torch.nn.modules.module.Module] = None)>,\n",
       "              'reparametrize': <function torch_geometric.nn.models.autoencoder.VGAE.reparametrize(self, mu: torch.Tensor, logstd: torch.Tensor) -> torch.Tensor>,\n",
       "              'encode': <function torch_geometric.nn.models.autoencoder.VGAE.encode(self, *args, **kwargs) -> torch.Tensor>,\n",
       "              'kl_loss': <function torch_geometric.nn.models.autoencoder.VGAE.kl_loss(self, mu: Optional[torch.Tensor] = None, logstd: Optional[torch.Tensor] = None) -> torch.Tensor>})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VGAE.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels, out_channels):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_channels)\n",
    "        self.conv_mu = GCNConv(hidden_channels, out_channels)\n",
    "        self.conv_logstd = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv_mu(x, edge_index), self.conv_logstd(x, edge_index)\n",
    "\n",
    "def generate_correlated_params(n, correlation):\n",
    "    mean = [0, 0]\n",
    "    cov = [[1, correlation], [correlation, 1]]\n",
    "    return np.random.multivariate_normal(mean, cov, n)\n",
    "\n",
    "def generate_graph(n, p):\n",
    "    return nx.erdos_renyi_graph(n, p)\n",
    "\n",
    "def get_embedding(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model.encode(data.x, data.edge_index)\n",
    "    return z.mean(dim=0).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment...\n",
      "Starting experiment...\n",
      "Generated 200 pairs of correlated parameters\n",
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating graph pairs: 100%|██████████| 200/200 [00:04<00:00, 41.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 400 graphs\n",
      "Training VGAE models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:   0%|          | 0/100 [01:08<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 92\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m param_corr, emb_corr\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning experiment...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 92\u001b[0m param_corr, emb_corr \u001b[38;5;241m=\u001b[39m \u001b[43mexperiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 54\u001b[0m, in \u001b[0;36mexperiment\u001b[0;34m(num_graphs, nodes, hidden_channels, out_channels, correlation)\u001b[0m\n\u001b[1;32m     52\u001b[0m optimizer1\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     53\u001b[0m z1 \u001b[38;5;241m=\u001b[39m model1\u001b[38;5;241m.\u001b[39mencode(data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index)\n\u001b[0;32m---> 54\u001b[0m loss1 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecon_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m loss1 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m data\u001b[38;5;241m.\u001b[39mnum_nodes) \u001b[38;5;241m*\u001b[39m model1\u001b[38;5;241m.\u001b[39mkl_loss()\n\u001b[1;32m     56\u001b[0m loss1\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniforge3/envs/gce/lib/python3.10/site-packages/torch_geometric/nn/models/autoencoder.py:103\u001b[0m, in \u001b[0;36mGAE.recon_loss\u001b[0;34m(self, z, pos_edge_index, neg_edge_index)\u001b[0m\n\u001b[1;32m     99\u001b[0m pos_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlog(\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(z, pos_edge_index, sigmoid\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m EPS)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m neg_edge_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 103\u001b[0m     neg_edge_index \u001b[38;5;241m=\u001b[39m \u001b[43mnegative_sampling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos_edge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m neg_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m\n\u001b[1;32m    105\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(z, neg_edge_index, sigmoid\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    106\u001b[0m                       EPS)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pos_loss \u001b[38;5;241m+\u001b[39m neg_loss\n",
      "File \u001b[0;32m~/miniforge3/envs/gce/lib/python3.10/site-packages/torch_geometric/utils/negative_sampling.py:65\u001b[0m, in \u001b[0;36mnegative_sampling\u001b[0;34m(edge_index, num_nodes, num_neg_samples, method, force_undirected)\u001b[0m\n\u001b[1;32m     62\u001b[0m size \u001b[38;5;241m=\u001b[39m (size, size) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m bipartite \u001b[38;5;28;01melse\u001b[39;00m size\n\u001b[1;32m     63\u001b[0m force_undirected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m bipartite \u001b[38;5;28;01melse\u001b[39;00m force_undirected\n\u001b[0;32m---> 65\u001b[0m idx, population \u001b[38;5;241m=\u001b[39m \u001b[43medge_index_to_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbipartite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mforce_undirected\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m idx\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m population:\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m edge_index\u001b[38;5;241m.\u001b[39mnew_empty((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/miniforge3/envs/gce/lib/python3.10/site-packages/torch_geometric/utils/negative_sampling.py:343\u001b[0m, in \u001b[0;36medge_index_to_vector\u001b[0;34m(edge_index, size, bipartite, force_undirected)\u001b[0m\n\u001b[1;32m    341\u001b[0m row, col \u001b[38;5;241m=\u001b[39m row[mask], col[mask]\n\u001b[1;32m    342\u001b[0m col[row \u001b[38;5;241m<\u001b[39m col] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 343\u001b[0m idx \u001b[38;5;241m=\u001b[39m \u001b[43mrow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39madd_(col)\n\u001b[1;32m    344\u001b[0m population \u001b[38;5;241m=\u001b[39m num_nodes \u001b[38;5;241m*\u001b[39m num_nodes \u001b[38;5;241m-\u001b[39m num_nodes\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m idx, population\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "def experiment(num_graphs=200, nodes=50, hidden_channels=32, out_channels=16, correlation=0.5):\n",
    "    print(\"Starting experiment...\")\n",
    "    # Generate correlated parameters\n",
    "    params = generate_correlated_params(num_graphs, correlation)\n",
    "    # Normalize params with sigmoid\n",
    "    params = torch.sigmoid(torch.tensor(params))\n",
    "    print(f\"Generated {num_graphs} pairs of correlated parameters\")\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "    device = 'mps'\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Generate graphs and prepare dataset\n",
    "    all_data = []\n",
    "    for i, (p1, p2) in enumerate(tqdm(params, desc=\"Generating graph pairs\")):\n",
    "        g1 = generate_graph(nodes, p1)\n",
    "        g2 = generate_graph(nodes, p2)\n",
    "        \n",
    "        data1 = from_networkx(g1)\n",
    "        data2 = from_networkx(g2)\n",
    "        \n",
    "        data1.x = torch.tensor(list(dict(g1.degree()).values()), dtype=torch.float).view(-1, 1).to(device)\n",
    "        data2.x = torch.tensor(list(dict(g2.degree()).values()), dtype=torch.float).view(-1, 1).to(device)\n",
    "        \n",
    "        data1.edge_index = data1.edge_index.to(device)\n",
    "        data2.edge_index = data2.edge_index.to(device)\n",
    "        \n",
    "        all_data.extend([data1, data2])\n",
    "    \n",
    "    print(f\"Generated {len(all_data)} graphs\")\n",
    "\n",
    "    # Create VGAE models\n",
    "    model1 = VGAE(Encoder(1, hidden_channels, out_channels)).to(device)\n",
    "    model2 = VGAE(Encoder(1, hidden_channels, out_channels)).to(device)\n",
    "\n",
    "    optimizer1 = torch.optim.Adam(model1.parameters(), lr=0.01)\n",
    "    optimizer2 = torch.optim.Adam(model2.parameters(), lr=0.01)\n",
    "\n",
    "    # Train VGAE models on the entire dataset\n",
    "    print(\"Training VGAE models...\")\n",
    "    for epoch in tqdm(range(100), desc=\"Training epochs\"):\n",
    "        model1.train()\n",
    "        model2.train()\n",
    "        total_loss1 = 0\n",
    "        total_loss2 = 0\n",
    "        \n",
    "        for data in all_data:\n",
    "            optimizer1.zero_grad()\n",
    "            z1 = model1.encode(data.x, data.edge_index)\n",
    "            loss1 = model1.recon_loss(z1, data.edge_index)\n",
    "            loss1 += (1 / data.num_nodes) * model1.kl_loss()\n",
    "            loss1.backward()\n",
    "            optimizer1.step()\n",
    "            total_loss1 += loss1.item()\n",
    "            \n",
    "            optimizer2.zero_grad()\n",
    "            z2 = model2.encode(data.x, data.edge_index)\n",
    "            loss2 = model2.recon_loss(z2, data.edge_index)\n",
    "            loss2 += (1 / data.num_nodes) * model2.kl_loss()\n",
    "            loss2.backward()\n",
    "            optimizer2.step()\n",
    "            total_loss2 += loss2.item()\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Avg Loss1: {total_loss1/len(all_data):.4f}, Avg Loss2: {total_loss2/len(all_data):.4f}\")\n",
    "\n",
    "    print(\"VGAE models trained\")\n",
    "\n",
    "    # Get embeddings for all graphs using the trained models\n",
    "    embeddings1 = []\n",
    "    embeddings2 = []\n",
    "    \n",
    "    print(\"Calculating embeddings...\")\n",
    "    for data in tqdm(all_data, desc=\"Processing graphs\"):\n",
    "        emb1 = get_embedding(model1, data)\n",
    "        emb2 = get_embedding(model2, data)\n",
    "        embeddings1.append(emb1)\n",
    "        embeddings2.append(emb2)\n",
    "\n",
    "    print(\"Calculating correlations...\")\n",
    "    # Calculate correlations\n",
    "    param_corr, _ = spearmanr(params.cpu().numpy()[:, 0], params.cpu().numpy()[:, 1])\n",
    "    emb_corr, _ = spearmanr(np.array(embeddings1).flatten(), np.array(embeddings2).flatten())\n",
    "\n",
    "    return param_corr, emb_corr\n",
    "\n",
    "print(\"Running experiment...\")\n",
    "param_corr, emb_corr = experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter correlation: 0.437911947798695\n",
      "Embedding correlation: -0.4264705882352941\n",
      "Experiment completed.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Parameter correlation: {param_corr}\")\n",
    "print(f\"Embedding correlation: {emb_corr}\")\n",
    "print(\"Experiment completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
