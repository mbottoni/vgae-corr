{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import VGAE, GCNConv\n",
    "from torch_geometric.utils import from_networkx, erdos_renyi_graph\n",
    "import networkx as nx\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mappingproxy({'__module__': 'torch_geometric.nn.models.autoencoder',\n",
       "              '__doc__': 'The Variational Graph Auto-Encoder model from the\\n    `\"Variational Graph Auto-Encoders\" <https://arxiv.org/abs/1611.07308>`_\\n    paper.\\n\\n    Args:\\n        encoder (torch.nn.Module): The encoder module to compute :math:`\\\\mu`\\n            and :math:`\\\\log\\\\sigma^2`.\\n        decoder (torch.nn.Module, optional): The decoder module. If set to\\n            :obj:`None`, will default to the\\n            :class:`torch_geometric.nn.models.InnerProductDecoder`.\\n            (default: :obj:`None`)\\n    ',\n",
       "              '__init__': <function torch_geometric.nn.models.autoencoder.VGAE.__init__(self, encoder: torch.nn.modules.module.Module, decoder: Optional[torch.nn.modules.module.Module] = None)>,\n",
       "              'reparametrize': <function torch_geometric.nn.models.autoencoder.VGAE.reparametrize(self, mu: torch.Tensor, logstd: torch.Tensor) -> torch.Tensor>,\n",
       "              'encode': <function torch_geometric.nn.models.autoencoder.VGAE.encode(self, *args, **kwargs) -> torch.Tensor>,\n",
       "              'kl_loss': <function torch_geometric.nn.models.autoencoder.VGAE.kl_loss(self, mu: Optional[torch.Tensor] = None, logstd: Optional[torch.Tensor] = None) -> torch.Tensor>})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VGAE.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels, out_channels):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_channels)\n",
    "        self.conv_mu = GCNConv(hidden_channels, out_channels)\n",
    "        self.conv_logstd = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv_mu(x, edge_index), self.conv_logstd(x, edge_index)\n",
    "\n",
    "def generate_correlated_params(n, correlation):\n",
    "    mean = [0, 0]\n",
    "    cov = [[1, correlation], [correlation, 1]]\n",
    "    return np.random.multivariate_normal(mean, cov, n)\n",
    "\n",
    "def generate_graph(n, p):\n",
    "    return nx.erdos_renyi_graph(n, p)\n",
    "\n",
    "def get_embedding(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model.encode(data.x, data.edge_index)\n",
    "    return z.mean(dim=0).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment...\n",
      "Starting experiment...\n",
      "Generated 200 pairs of correlated parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating graph pairs:  20%|██        | 40/200 [00:00<00:04, 37.90it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def experiment(num_graphs=200, nodes=50, hidden_channels=32, out_channels=16, correlation=0.5):\n",
    "    print(\"Starting experiment...\")\n",
    "    # Generate correlated parameters\n",
    "    params = generate_correlated_params(num_graphs, correlation)\n",
    "    # Normalize params with sigmoid\n",
    "    params = torch.sigmoid(torch.tensor(params))\n",
    "    print(f\"Generated {num_graphs} pairs of correlated parameters\")\n",
    "    \n",
    "    # Generate graphs and prepare dataset\n",
    "    all_data = []\n",
    "    for i, (p1, p2) in enumerate(tqdm(params, desc=\"Generating graph pairs\")):\n",
    "        g1 = generate_graph(nodes, p1)\n",
    "        g2 = generate_graph(nodes, p2)\n",
    "        \n",
    "        data1 = from_networkx(g1)\n",
    "        data2 = from_networkx(g2)\n",
    "        \n",
    "        data1.x = torch.tensor(list(dict(g1.degree()).values()), dtype=torch.float).view(-1, 1)\n",
    "        data2.x = torch.tensor(list(dict(g2.degree()).values()), dtype=torch.float).view(-1, 1)\n",
    "        \n",
    "        all_data.extend([data1, data2])\n",
    "    \n",
    "    print(f\"Generated {len(all_data)} graphs\")\n",
    "\n",
    "    # Create VGAE models\n",
    "    model1 = VGAE(Encoder(1, hidden_channels, out_channels))\n",
    "    model2 = VGAE(Encoder(1, hidden_channels, out_channels))\n",
    "\n",
    "    optimizer1 = torch.optim.Adam(model1.parameters(), lr=0.01)\n",
    "    optimizer2 = torch.optim.Adam(model2.parameters(), lr=0.01)\n",
    "\n",
    "    # Train VGAE models on the entire dataset\n",
    "    print(\"Training VGAE models...\")\n",
    "    for epoch in tqdm(range(100), desc=\"Training epochs\"):\n",
    "        model1.train()\n",
    "        model2.train()\n",
    "        total_loss1 = 0\n",
    "        total_loss2 = 0\n",
    "        \n",
    "        for data in all_data:\n",
    "            optimizer1.zero_grad()\n",
    "            z1 = model1.encode(data.x, data.edge_index)\n",
    "            loss1 = model1.recon_loss(z1, data.edge_index)\n",
    "            loss1 += (1 / data.num_nodes) * model1.kl_loss()\n",
    "            loss1.backward()\n",
    "            optimizer1.step()\n",
    "            total_loss1 += loss1.item()\n",
    "            \n",
    "            optimizer2.zero_grad()\n",
    "            z2 = model2.encode(data.x, data.edge_index)\n",
    "            loss2 = model2.recon_loss(z2, data.edge_index)\n",
    "            loss2 += (1 / data.num_nodes) * model2.kl_loss()\n",
    "            loss2.backward()\n",
    "            optimizer2.step()\n",
    "            total_loss2 += loss2.item()\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Avg Loss1: {total_loss1/len(all_data):.4f}, Avg Loss2: {total_loss2/len(all_data):.4f}\")\n",
    "\n",
    "    print(\"VGAE models trained\")\n",
    "\n",
    "    # Get embeddings for all graphs using the trained models\n",
    "    embeddings1 = []\n",
    "    embeddings2 = []\n",
    "    \n",
    "    print(\"Calculating embeddings...\")\n",
    "    for data in tqdm(all_data, desc=\"Processing graphs\"):\n",
    "        emb1 = get_embedding(model1, data)\n",
    "        emb2 = get_embedding(model2, data)\n",
    "        embeddings1.append(emb1)\n",
    "        embeddings2.append(emb2)\n",
    "\n",
    "    print(\"Calculating correlations...\")\n",
    "    # Calculate correlations\n",
    "    param_corr, _ = spearmanr(params[:, 0], params[:, 1])\n",
    "    emb_corr, _ = spearmanr(np.array(embeddings1).flatten(), np.array(embeddings2).flatten())\n",
    "\n",
    "    return param_corr, emb_corr\n",
    "\n",
    "print(\"Running experiment...\")\n",
    "param_corr, emb_corr = experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter correlation: -1.0\n",
      "Embedding correlation: -0.020076036018167577\n",
      "Experiment completed.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Parameter correlation: {param_corr}\")\n",
    "print(f\"Embedding correlation: {emb_corr}\")\n",
    "print(\"Experiment completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
