{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import VGAE, GCNConv\n",
    "from torch_geometric.utils import from_networkx, erdos_renyi_graph\n",
    "import networkx as nx\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mappingproxy({'__module__': 'torch_geometric.nn.models.autoencoder',\n",
       "              '__doc__': 'The Variational Graph Auto-Encoder model from the\\n    `\"Variational Graph Auto-Encoders\" <https://arxiv.org/abs/1611.07308>`_\\n    paper.\\n\\n    Args:\\n        encoder (torch.nn.Module): The encoder module to compute :math:`\\\\mu`\\n            and :math:`\\\\log\\\\sigma^2`.\\n        decoder (torch.nn.Module, optional): The decoder module. If set to\\n            :obj:`None`, will default to the\\n            :class:`torch_geometric.nn.models.InnerProductDecoder`.\\n            (default: :obj:`None`)\\n    ',\n",
       "              '__init__': <function torch_geometric.nn.models.autoencoder.VGAE.__init__(self, encoder: torch.nn.modules.module.Module, decoder: Optional[torch.nn.modules.module.Module] = None)>,\n",
       "              'reparametrize': <function torch_geometric.nn.models.autoencoder.VGAE.reparametrize(self, mu: torch.Tensor, logstd: torch.Tensor) -> torch.Tensor>,\n",
       "              'encode': <function torch_geometric.nn.models.autoencoder.VGAE.encode(self, *args, **kwargs) -> torch.Tensor>,\n",
       "              'kl_loss': <function torch_geometric.nn.models.autoencoder.VGAE.kl_loss(self, mu: Optional[torch.Tensor] = None, logstd: Optional[torch.Tensor] = None) -> torch.Tensor>})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VGAE.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels, out_channels):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_channels)\n",
    "        self.conv_mu = GCNConv(hidden_channels, out_channels)\n",
    "        self.conv_logstd = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv_mu(x, edge_index), self.conv_logstd(x, edge_index)\n",
    "\n",
    "def generate_correlated_params(n, correlation):\n",
    "    mean = [0, 0]\n",
    "    cov = [[1, correlation], [correlation, 1]]\n",
    "    return np.random.multivariate_normal(mean, cov, n)\n",
    "\n",
    "def generate_graph(n, p):\n",
    "    return nx.erdos_renyi_graph(n, p)\n",
    "\n",
    "def get_embedding(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model.encode(data.x, data.edge_index)\n",
    "    return z.mean(dim=0).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(num_graphs=100, nodes=50, hidden_channels=32, out_channels=16, correlation=0.5):\n",
    "    # Generate correlated parameters\n",
    "    params = generate_correlated_params(num_graphs, correlation)\n",
    "    params = torch.softmax(torch.tensor(params), dim=1).numpy()  # Normalize using softmax\n",
    "    # Generate graphs and get VGAE embeddings\n",
    "    embeddings1 = []\n",
    "    embeddings2 = []\n",
    "\n",
    "    for p1, p2 in params:\n",
    "        # Generate two graphs\n",
    "        g1 = generate_graph(nodes, p1)\n",
    "        g2 = generate_graph(nodes, p2)\n",
    "\n",
    "        # Convert to PyTorch Geometric data objects\n",
    "        data1 = from_networkx(g1)\n",
    "        data2 = from_networkx(g2)\n",
    "\n",
    "        # Add node features (here we use degree as a simple feature)\n",
    "        data1.x = torch.tensor(list(dict(g1.degree()).values()), dtype=torch.float).view(-1, 1)\n",
    "        data2.x = torch.tensor(list(dict(g2.degree()).values()), dtype=torch.float).view(-1, 1)\n",
    "\n",
    "        # Create and train VGAE models\n",
    "        model1 = VGAE(Encoder(1, hidden_channels, out_channels))\n",
    "        model2 = VGAE(Encoder(1, hidden_channels, out_channels))\n",
    "\n",
    "        optimizer1 = torch.optim.Adam(model1.parameters(), lr=0.01)\n",
    "        optimizer2 = torch.optim.Adam(model2.parameters(), lr=0.01)\n",
    "\n",
    "        for epoch in range(100):\n",
    "            model1.train()\n",
    "            optimizer1.zero_grad()\n",
    "            z1 = model1.encode(data1.x, data1.edge_index)\n",
    "            loss1 = model1.recon_loss(z1, data1.edge_index)\n",
    "            loss1 += (1 / data1.num_nodes) * model1.kl_loss()\n",
    "            loss1.backward()\n",
    "            optimizer1.step()\n",
    "\n",
    "            model2.train()\n",
    "            optimizer2.zero_grad()\n",
    "            z2 = model2.encode(data2.x, data2.edge_index)\n",
    "            loss2 = model2.recon_loss(z2, data2.edge_index)\n",
    "            loss2 += (1 / data2.num_nodes) * model2.kl_loss()\n",
    "            loss2.backward()\n",
    "            optimizer2.step()\n",
    "\n",
    "        # Get embeddings\n",
    "        emb1 = get_embedding(model1, data1)\n",
    "        emb2 = get_embedding(model2, data2)\n",
    "\n",
    "        embeddings1.append(emb1)\n",
    "        embeddings2.append(emb2)\n",
    "\n",
    "    # Calculate correlations\n",
    "    param_corr, _ = spearmanr(params[:, 0], params[:, 1])\n",
    "    emb_corr, _ = spearmanr(np.array(embeddings1).flatten(), np.array(embeddings2).flatten())\n",
    "\n",
    "    return param_corr, emb_corr\n",
    "\n",
    "param_corr, emb_corr = experiment()\n",
    "print(f\"Parameter correlation: {param_corr}\")\n",
    "print(f\"Embedding correlation: {emb_corr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
